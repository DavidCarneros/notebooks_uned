{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tema 6 estadistica.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOP0VjNOYMvF4Yrk3M4UXR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidCarneros/notebooks_uned/blob/main/Tema_6_estadistica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaUUlmF6Y0tu"
      },
      "source": [
        "# Tema 6. Variantes del modelo de regresión lineal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_aMpEjPa8fm"
      },
      "source": [
        "## Métodos de contracción \n",
        "\n",
        "Los métodos de selección de subconjuntos implican el uso de mínimos cuadrados para ajustar un modelo lineal que contiene un subconjunto de predictores. Como alternativa, podemos ajustar un modelo que contenga todos los $p$ predictores utilizando una técnica que restrinja o regularice las estimaciones de coeficientes, o de manera equivalente, que reduzca las estimaciones de coeficientes a cero. Puede que no sea inmediatamente obvio por qué tal restricción debería mejorar el ajuste, pero resulta que la reducción de las estimaciones de coeficientes puede reducir significativamente su varianza. Las dos técnicas más conocidas para reducir los coeficientes de regresión hacia cero son la regresión de **cresta** (ridge) y el **lasso**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuX2j1Ica8kw"
      },
      "source": [
        "### Regresión de cresta (Ridge Regression)\n",
        "\n",
        "Recordando el procedimiento de ajuste por mínimos cuadrados que estima $\\beta_0, \\beta_1, ..., \\beta_p$ utilizando los valores que minimizan:\n",
        "\n",
        "$$ RSS = \\sum_{i=1}^n \\left( y_i - \\beta_0- \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2  $$\n",
        "\n",
        "La regresión de crestas es muy similar a los mínimos cuadrados, excepto que los coeficientes se estiman minimizando una cantidad ligeramente diferente. En particular, las estimaciones del coeficiente de regresión de la cresta $\\hat{\\beta}^R$ son los valores que minimizan:\n",
        "\n",
        "$$ RSS = \\sum_{i=1}^n \\left( y_i - \\beta_0- \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta^2_j = RSS + \\lambda \\sum_{j=1}^p \\beta^2_j$$\n",
        "\n",
        "donde $\\lambda \\geq 0 $ es un **parámetro de ajuste**, que se determinará por separado. Al igual que con los *mínimos cuadrados*, la *regresión de crestas* busca estimaciones de coeficientes que se ajusten bien a los datos, haciendo que el $RSS$ sea pequeño. Sin embargo, el segundo término, $\\lambda \\sum_j \\beta^2_j$, llamado **penalización por contracción** (shrinkage penalty), es pequeño cuando $\\beta_1, ..., \\beta_p$ están cerca de cero, por lo que tiene el efecto de reducir las estimaciones de $\\beta_j$ hacia cero. El parámetro de ajuste $\\lambda$ sirve para controlar el impacto relativo de estos dos términos en las estimaciones del coeficiente de regresión. Cuando $\\lambda = 0$, el término de penalización no tiene ningún efecto y la regresión de crestas producirá estimaciones de mínimos cuadrados. Sin embargo, a medida que $\\lambda \\longrightarrow \\infty$, el impacto de la penalización por contracción aumenta y las estimaciones del coeficiente de regresión de la cresta se acercarán a cero. A diferencia de los mínimos cuadrados, que genera solo un conjunto de estimaciones de coeficientes, la regresión de crestas producirá un conjunto diferente de estimaciones de coeficientes, $\\hat{\\beta}^R_\\lambda$, para cada valor de $\\lambda$. Queremos reducir la asociación estimada de cada variable con la respuesta; sin embargo, no queremos reducir la intersección, que es simplemente una medida del valor medio de la respuesta cuando $x_{i1} = x_{i2} = ... = x_{ip} = 0$. Si asumimos que las variables, es decir, las columnas de la matriz de datos $X$: se han centrado para tener una media cero antes de que se realice la regresión de crestas, entonces la intersección estimada tomará la forma $\\hat{\\beta}_0 = \\bar{y} = \\sum_{i=1}^n y_i/n$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6Da5u-Ra8yH"
      },
      "source": [
        "#### Una aplicación a los datos crediticios \n",
        "\n",
        "En la Figura 1, se muestran las estimaciones del coeficiente de regresión de la cresta para el conjunto de datos **Credit**. En el panel de la izquierda, cada curva corresponde a la estimación del coeficiente de regresión de la cresta para una de las diez variables, representada como una función de $\\lambda$. Por ejemplo, la línea negra sólida representa la estimación de la regresión de la cresta para el coeficiente de **ingresos**, ya que $\\lambda$ varía. En el extremo izquierdo del gráfico, $\\lambda$ es esencialmente cero, por lo que las estimaciones de coeficientes de cresta correspondientes son las mismas que las estimaciones de mínimos cuadrados habituales. Pero a medida que aumenta $\\lambda$, las estimaciones del coeficiente de cresta se reducen a cero. Cuando $\\lambda$ es extremadamente grande, entonces todas las estimaciones del coeficiente de cresta son básicamente cero; esto corresponde al modelo nulo que no contiene predictores. En este gráfico, las variables de **ingresos**, **límites**, **calificaciones** y **estudiantes** se muestran en distintos colores, ya que estas variables tienden a tener, con mucho, las estimaciones de coeficientes más grandes. Si bien las estimaciones del coeficiente de cresta tienden a disminuir en conjunto a medida que aumenta $\\lambda$, los coeficientes individuales, como la **calificación** y los **ingresos**, pueden ocasionalmente aumentar a medida que aumenta $\\lambda$.\n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/ce16745b9455d57b83a04f917060ddc1.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.1</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "El panel de la derecha de la Figura 1 muestra las mismas estimaciones de coeficientes de cresta que el panel de la izquierda, pero en lugar de mostrar $\\lambda$ en el eje $x$, ahora mostramos $ ||\\hat{\\beta}^R_\\lambda||_2 /|| \\hat{\\beta}||_2 $, donde $\\hat{\\beta}$ denota el vector de estimaciones de coeficientes de mínimos cuadrados. La notación $||\\beta||_2$ denota la $\\ell_2$ norm de un vector, y se define como $||\\beta||_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}$. Mide la distancia de $\\beta$ desde cero. A medida que aumenta $\\lambda$, la  $\\ell_2$ norm de $\\hat{\\beta}^R_\\lambda$ siempre disminuirá, y también lo hará $ ||\\hat{\\beta}^R_\\lambda||_2 /|| \\hat{\\beta}||_2 $. La última cantidad varía de 1 (cuando $\\lambda = 0$, en cuyo caso la estimación del coeficiente de regresión de la cresta es la misma que la estimación de mínimos cuadrados, por lo que sus $\\ell_2$ norm son iguales) a 0 (cuando $\\lambda = \\infty$, en cuyo caso la cresta la estimación del coeficiente de regresión es un factor de ceros, con $\\ell_2$ norm igual a cero). Por lo tanto, podemos pensar en el eje x en el panel de la derecha de la Figura 1 como la cantidad en que las estimaciones del coeficiente de regresión de la cresta se han reducido a cero; un valor pequeño indica que se han reducido muy cerca de cero.\n",
        "\n",
        "Las estimaciones de coeficientes de mínimos cuadrados estándar son equivariantes de escala: multiplicar $X_j$ por una constante $c$ simplemente conduce a una escala de las estimaciones de coeficientes de mínimos cuadrados por un factor de $1 / c$. En otras palabras, independientemente de cómo se escala el j-ésimo predictor, $X_j \\hat{\\beta}_j$ permanece igual. Por el contrario, las estimaciones del coeficiente de regresión de la cresta pueden cambiar sustancialmente cuando se multiplica un predictor dado por una constante. $X_j \\hat{\\beta}^R_{j,\\lambda}$ dependerá no solo del valor de $\\lambda$, sino también de la escala del j-ésimo predictor. De hecho, ¡el valor de $X_j \\hat{\\beta}^R_{j,\\lambda}$ puede incluso depender de la escala de los otros predictores! Por lo tanto, es mejor aplicar la regresión de crestas después de **estandarizar los predictores**, usando la fórmula\n",
        "\n",
        "$$ \\tilde{x}_{ij}  = \\frac {x_{ij}}  {\\sqrt{\\frac{1}{n}\\sum^n_{i=1}(x_{ij}-\\bar{x}_j})^2} $$\n",
        "\n",
        "para que estén todos en la misma escala. En la formula superior, el denominador es la desviación estándar estimada del j-ésimo predictor. En consecuencia, todos los predictores estandarizados tendrán una desviación estándar de uno. Como resultado, el ajuste final no dependerá de la escala en la que se midan los predictores. En la Figura 1, el eje y muestra las estimaciones de coeficientes de regresión de crestas estandarizadas, es decir, las estimaciones de coeficientes que resultan de realizar la regresión de crestas utilizando predictores estandarizados.\n",
        "\n",
        "#### ¿Por qué mejora la regresión de crestas con respecto a los mínimos cuadrados?\n",
        "\n",
        "La ventaja de la regresión de crestas sobre los mínimos cuadrados se basa en la compensación de sesgo-varianza. A medida que aumenta $\\lambda$, la flexibilidad del ajuste de regresión de crestas disminuye, lo que lleva a una menor varianza pero a un mayor sesgo. Esto se ilustra en el panel de la izquierda de la Figura 2, utilizando un conjunto de datos simulados que contiene p = 45 predictores y n = 50 observaciones. La curva verde en el panel de la izquierda de la Figura 2 muestra la varianza de las predicciones de regresión de la cresta como una función de $\\lambda$. En las estimaciones del coeficiente de mínimos cuadrados, que corresponden a la regresión de crestas con $\\lambda = 0$, la varianza es alta pero no hay sesgo. Pero a medida que aumenta $\\lambda$, la contracción de las estimaciones del coeficiente de cresta conduce a una reducción sustancial en la varianza de las predicciones, a expensas de un ligero aumento del sesgo. Recuerde que el error cuadrático medio de la prueba (MSE), representado en púrpura, es una función de la varianza más el sesgo cuadrático. Para valores de $\\lambda$ hasta aproximadamente 10, la varianza disminuye rápidamente, con muy poco aumento en el sesgo, representado en negro. En consecuencia, el MSE cae considerablemente a medida que $\\lambda$ aumenta de 0 a 10. Más allá de este punto, la disminución de la varianza debido al aumento de $\\lambda$ se ralentiza y la contracción de los coeficientes hace que se subestimen significativamente, lo que da como resultado un gran aumento en el sesgo. El MSE mínimo se logra aproximadamente en $\\lambda = 30$. Curiosamente, debido a su alta varianza, el MSE asociado con el ajuste por mínimos cuadrados, cuando $\\lambda = 0$, es casi tan alto como el del modelo nulo para el que todas las estimaciones de coeficientes son cero, cuando $\\lambda = \\infty$. Sin embargo, para un valor intermedio de $\\lambda$, el MSE es considerablemente menor\n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/2c271cb0c8db996449d03b37e1aef9dc.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.2</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "El panel de la derecha de la Figura 2 muestra las mismas curvas que el panel de la izquierda, esta vez graficadas contra la norma $\\ell_2$ de las estimaciones del coeficiente de regresión de la cresta dividida por la norma $\\ell_2$ de las estimaciones de mínimos cuadrados.\n",
        "\n",
        "En general, en situaciones en las que la relación entre la respuesta y los predictores es casi lineal, las estimaciones de mínimos cuadrados tendrán un sesgo bajo pero pueden tener una gran varianza. Esto significa que un pequeño cambio en los datos de entrenamiento puede provocar un gran cambio en las estimaciones del coeficiente de mínimos cuadrados. En particular, cuando el número de variables $p$ es casi tan grande como el número de observaciones $n$, como en el ejemplo de la Figura 2, las estimaciones de mínimos cuadrados serán extremadamente variables. Y si $p > n$, entonces las estimaciones de mínimos cuadrados ni siquiera tienen una solución única, mientras que la regresión de crestas aún puede funcionar bien intercambiando un pequeño aumento en el sesgo por una gran disminución en la varianza. Por lo tanto, la regresión de crestas funciona mejor en situaciones donde las estimaciones de mínimos cuadrados tienen una alta varianza.\n",
        "\n",
        "La regresión de crestas también tiene ventajas computacionales sustanciales sobre la mejor selección de subconjuntos, que requiere la búsqueda a través de modelos $2^p$. Como discutimos anteriormente, incluso para valores moderados de $p$, tal búsqueda puede ser computacionalmente inviable. Por el contrario, para cualquier valor fijo de $\\lambda$, la regresión de crestas solo se ajusta a un único modelo, y el procedimiento de ajuste del modelo se puede realizar con bastante rapidez. De hecho, se puede demostrar que los cálculos necesarios para resolver (figura 2), simultáneamente para todos los valores de $\\lambda$, son casi idénticos a los que se utilizan para ajustar un modelo mediante mínimos cuadrados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zUPIiB5a8vv"
      },
      "source": [
        "### El lasso\n",
        "\n",
        "La regresión de crestas tiene una desventaja obvia. A diferencia de la selección de mejor subconjunto, paso hacia adelante y paso hacia atrás, que generalmente seleccionará modelos que involucran solo un subconjunto de las variables, la regresión de crestas incluirá todos los predictores $p$ en el modelo final. La penalización $\\lambda\\sum\\beta^2_j$ en la regresión de cresta reducirá todos los coeficientes hacia cero, pero no establecerá ninguno de ellos exactamente en cero (a menos que $\\lambda = \\infty$). Esto puede no ser un problema para la precisión de la predicción, pero puede crear un desafío en la interpretación del modelo en entornos en los que el número de variables es bastante grande. Por ejemplo, en el conjunto de datos de **crédito**, parece que las variables más importantes son el **ingreso**, el **límite**, la **calificación** y el **estudiante**, por lo que es posible que deseemos construir un modelo que incluya solo estos predictores. Sin embargo, la regresión de crestas siempre generará un modelo que incluya los diez predictores. Incrementar el valor de $\\lambda$ tenderá a reducir las magnitudes de los coeficientes, pero no resultará en la exclusión de ninguna de las variables.\n",
        "\n",
        "El lasso es una alternativa relativamente reciente a la regresión de la cresta que supera esta desventaja. Los coeficientes de lazo, $\\hat{\\beta}^L_\\lambda$, minimizan la cantidad\n",
        "\n",
        "$$ \\sum^n_{i=1} = \\left ( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right )^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j| $$\n",
        "\n",
        "vemos que la regresión de lasso y cresta tienen formulaciones similares. La única diferencia es que el término $\\beta_j^2$ en la penalización por regresión de la cresta ha sido reemplazado por $|\\beta_j|$ en la penalización de lasso. En el lenguaje estadístico, el lasso usa una penalización de $\\ell_1$ en lugar de una penalización de $\\ell_2$. La norma $\\ell_1$ de un vector de coeficientes $\\beta$ viene dada por $||\\beta||_1= \\sum|\\beta_j|$.\n",
        "\n",
        "Al igual que con la regresión de crestas, el lasso reduce las estimaciones de coeficientes hacia cero. Sin embargo, en el caso del lasso, la penalización $\\ell_1$ tiene el efecto de forzar algunas de las estimaciones de coeficientes a ser exactamente iguales a cero cuando el parámetro de ajuste $\\lambda$ es suficientemente grande. Por lo tanto, al igual que la selección del mejor subconjunto, el lasso realiza la selección de variables. Como resultado, los modelos generados a partir del lasso son generalmente mucho más fáciles de interpretar que los producidos por regresión de crestas. Decimos que el lasso produce **modelos escasos**. Como en la regresión de crestas, es fundamental seleccionar un buen valor de $\\lambda$ para el lasso. \n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/60ac794bedc5969a82cf00bcf63eeeac.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.3</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "Como ejemplo, considere las gráficas de coeficientes en la Figura 3, que se generan al aplicar el lasso al conjunto de datos de **Crédito**. Cuando $\\lambda = 0$, entonces el lasso simplemente da el ajuste de mínimos cuadrados, y cuando $\\lambda$ se vuelve lo suficientemente grande, el lasso da el modelo nulo en el que todas las estimaciones de coeficientes son iguales a cero. Sin embargo, entre estos dos extremos, los modelos de regresión de cresta y lasso son bastante diferentes entre sí. Moviéndose de izquierda a derecha en el panel de la derecha de la Figura 3, observamos que al principio el lasso da como resultado un modelo que contiene solo el predictor de calificación. Luego, el alumno y el limitador ingresan al modelo casi simultáneamente, seguido poco después por los ingresos. Finalmente, las variables restantes ingresan al modelo. Por tanto, dependiendo del valor de $\\lambda$, el lasso puede producir un modelo que involucre cualquier número de variables. Por el contrario, la regresión de crestas siempre incluirá todas las variables del modelo, aunque la magnitud de las estimaciones de los coeficientes dependerá de $\\lambda$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVATKzrDa8tb"
      },
      "source": [
        "#### Otra formulación para la regresión de crestas (Ridge Regression) y el lasso\n",
        "\n",
        "**VOY POR AQUI (PAG 220)**\n",
        "\n",
        "#### La propiedad de selección de variable del lasso\n",
        "\n",
        "#### Comparación de la regresión de lasso y cresta (ridge)\n",
        "\n",
        "\n",
        "#### Un caso especial simple para la regresión de crestas y el lasso\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53MgC654a8q-"
      },
      "source": [
        "### Selección del parámetro de afinación "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv-peoeRa8oy"
      },
      "source": [
        "## Métodos de reducción de dimensiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11-JpSQJcciR"
      },
      "source": [
        "### Componentes Principales de regresión\n",
        "\n",
        "#### Una descripción general del análisis de componentes principales\n",
        "\n",
        "#### El enfoque a los componentes principales de regresión\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5lTvRj_ccn4"
      },
      "source": [
        "### Mínimos cuadrados parciales\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCu8XZpedXU-"
      },
      "source": [
        "### Consideraciones en grandes dimensiones \n",
        "\n",
        "#### Datos de alta dimensión\n",
        "\n",
        "#### ¿Qué sale mal en las dimensiones altas?\n",
        "\n",
        "#### Regresión en altas dimensiones\n",
        "\n",
        "#### Interpretación de resultados en grandes dimensiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viEOxPLNdXcA"
      },
      "source": [
        "## Lab: Regresión de crestas y lazo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qBxEhwDdXYt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVQkiRL0dXMJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv0JUpGWY3JN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}