{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tema 6 estadistica.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhU1pKE0SnzNjfNgwzxU61",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidCarneros/notebooks_uned/blob/main/Tema_6_estadistica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaUUlmF6Y0tu"
      },
      "source": [
        "# Tema 6. Variantes del modelo de regresión lineal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_aMpEjPa8fm"
      },
      "source": [
        "## Métodos de contracción \n",
        "\n",
        "Los métodos de selección de subconjuntos implican el uso de mínimos cuadrados para ajustar un modelo lineal que contiene un subconjunto de predictores. Como alternativa, podemos ajustar un modelo que contenga todos los $p$ predictores utilizando una técnica que restrinja o regularice las estimaciones de coeficientes, o de manera equivalente, que reduzca las estimaciones de coeficientes a cero. Puede que no sea inmediatamente obvio por qué tal restricción debería mejorar el ajuste, pero resulta que la reducción de las estimaciones de coeficientes puede reducir significativamente su varianza. Las dos técnicas más conocidas para reducir los coeficientes de regresión hacia cero son la regresión de **cresta** (ridge) y el **lasso**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuX2j1Ica8kw"
      },
      "source": [
        "### Regresión de cresta (Ridge Regression)\n",
        "\n",
        "Recordando el procedimiento de ajuste por mínimos cuadrados que estima $\\beta_0, \\beta_1, ..., \\beta_p$ utilizando los valores que minimizan: (eq1)\n",
        "\n",
        "$$ RSS = \\sum_{i=1}^n \\left( y_i - \\beta_0- \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2  $$\n",
        "\n",
        "La regresión de crestas es muy similar a los mínimos cuadrados, excepto que los coeficientes se estiman minimizando una cantidad ligeramente diferente. En particular, las estimaciones del coeficiente de regresión de la cresta $\\hat{\\beta}^R$ son los valores que minimizan: (eq2)\n",
        "\n",
        "$$ RSS = \\sum_{i=1}^n \\left( y_i - \\beta_0- \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta^2_j = RSS + \\lambda \\sum_{j=1}^p \\beta^2_j$$\n",
        "\n",
        "donde $\\lambda \\geq 0 $ es un **parámetro de ajuste**, que se determinará por separado. Al igual que con los *mínimos cuadrados*, la *regresión de crestas* busca estimaciones de coeficientes que se ajusten bien a los datos, haciendo que el $RSS$ sea pequeño. Sin embargo, el segundo término, $\\lambda \\sum_j \\beta^2_j$, llamado **penalización por contracción** (shrinkage penalty), es pequeño cuando $\\beta_1, ..., \\beta_p$ están cerca de cero, por lo que tiene el efecto de reducir las estimaciones de $\\beta_j$ hacia cero. El parámetro de ajuste $\\lambda$ sirve para controlar el impacto relativo de estos dos términos en las estimaciones del coeficiente de regresión. Cuando $\\lambda = 0$, el término de penalización no tiene ningún efecto y la regresión de crestas producirá estimaciones de mínimos cuadrados. Sin embargo, a medida que $\\lambda \\longrightarrow \\infty$, el impacto de la penalización por contracción aumenta y las estimaciones del coeficiente de regresión de la cresta se acercarán a cero. A diferencia de los mínimos cuadrados, que genera solo un conjunto de estimaciones de coeficientes, la regresión de crestas producirá un conjunto diferente de estimaciones de coeficientes, $\\hat{\\beta}^R_\\lambda$, para cada valor de $\\lambda$. Queremos reducir la asociación estimada de cada variable con la respuesta; sin embargo, no queremos reducir la intersección, que es simplemente una medida del valor medio de la respuesta cuando $x_{i1} = x_{i2} = ... = x_{ip} = 0$. Si asumimos que las variables, es decir, las columnas de la matriz de datos $X$: se han centrado para tener una media cero antes de que se realice la regresión de crestas, entonces la intersección estimada tomará la forma $\\hat{\\beta}_0 = \\bar{y} = \\sum_{i=1}^n y_i/n$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6Da5u-Ra8yH"
      },
      "source": [
        "#### Una aplicación a los datos crediticios \n",
        "\n",
        "En la Figura 1, se muestran las estimaciones del coeficiente de regresión de la cresta para el conjunto de datos **Credit**. En el panel de la izquierda, cada curva corresponde a la estimación del coeficiente de regresión de la cresta para una de las diez variables, representada como una función de $\\lambda$. Por ejemplo, la línea negra sólida representa la estimación de la regresión de la cresta para el coeficiente de **ingresos**, ya que $\\lambda$ varía. En el extremo izquierdo del gráfico, $\\lambda$ es esencialmente cero, por lo que las estimaciones de coeficientes de cresta correspondientes son las mismas que las estimaciones de mínimos cuadrados habituales. Pero a medida que aumenta $\\lambda$, las estimaciones del coeficiente de cresta se reducen a cero. Cuando $\\lambda$ es extremadamente grande, entonces todas las estimaciones del coeficiente de cresta son básicamente cero; esto corresponde al modelo nulo que no contiene predictores. En este gráfico, las variables de **ingresos**, **límites**, **calificaciones** y **estudiantes** se muestran en distintos colores, ya que estas variables tienden a tener, con mucho, las estimaciones de coeficientes más grandes. Si bien las estimaciones del coeficiente de cresta tienden a disminuir en conjunto a medida que aumenta $\\lambda$, los coeficientes individuales, como la **calificación** y los **ingresos**, pueden ocasionalmente aumentar a medida que aumenta $\\lambda$.\n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/ce16745b9455d57b83a04f917060ddc1.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.1</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "El panel de la derecha de la Figura 1 muestra las mismas estimaciones de coeficientes de cresta que el panel de la izquierda, pero en lugar de mostrar $\\lambda$ en el eje $x$, ahora mostramos $ ||\\hat{\\beta}^R_\\lambda||_2 /|| \\hat{\\beta}||_2 $, donde $\\hat{\\beta}$ denota el vector de estimaciones de coeficientes de mínimos cuadrados. La notación $||\\beta||_2$ denota la $\\ell_2$ norm de un vector, y se define como $||\\beta||_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}$. Mide la distancia de $\\beta$ desde cero. A medida que aumenta $\\lambda$, la  $\\ell_2$ norm de $\\hat{\\beta}^R_\\lambda$ siempre disminuirá, y también lo hará $ ||\\hat{\\beta}^R_\\lambda||_2 /|| \\hat{\\beta}||_2 $. La última cantidad varía de 1 (cuando $\\lambda = 0$, en cuyo caso la estimación del coeficiente de regresión de la cresta es la misma que la estimación de mínimos cuadrados, por lo que sus $\\ell_2$ norm son iguales) a 0 (cuando $\\lambda = \\infty$, en cuyo caso la cresta la estimación del coeficiente de regresión es un factor de ceros, con $\\ell_2$ norm igual a cero). Por lo tanto, podemos pensar en el eje x en el panel de la derecha de la Figura 1 como la cantidad en que las estimaciones del coeficiente de regresión de la cresta se han reducido a cero; un valor pequeño indica que se han reducido muy cerca de cero.\n",
        "\n",
        "Las estimaciones de coeficientes de mínimos cuadrados estándar son equivariantes de escala: multiplicar $X_j$ por una constante $c$ simplemente conduce a una escala de las estimaciones de coeficientes de mínimos cuadrados por un factor de $1 / c$. En otras palabras, independientemente de cómo se escala el j-ésimo predictor, $X_j \\hat{\\beta}_j$ permanece igual. Por el contrario, las estimaciones del coeficiente de regresión de la cresta pueden cambiar sustancialmente cuando se multiplica un predictor dado por una constante. $X_j \\hat{\\beta}^R_{j,\\lambda}$ dependerá no solo del valor de $\\lambda$, sino también de la escala del j-ésimo predictor. De hecho, ¡el valor de $X_j \\hat{\\beta}^R_{j,\\lambda}$ puede incluso depender de la escala de los otros predictores! Por lo tanto, es mejor aplicar la regresión de crestas después de **estandarizar los predictores**, usando la fórmula (eq3)\n",
        "\n",
        "$$ \\tilde{x}_{ij}  = \\frac {x_{ij}}  {\\sqrt{\\frac{1}{n}\\sum^n_{i=1}(x_{ij}-\\bar{x}_j})^2} $$\n",
        "\n",
        "para que estén todos en la misma escala. En la formula superior, el denominador es la desviación estándar estimada del j-ésimo predictor. En consecuencia, todos los predictores estandarizados tendrán una desviación estándar de uno. Como resultado, el ajuste final no dependerá de la escala en la que se midan los predictores. En la Figura 1, el eje y muestra las estimaciones de coeficientes de regresión de crestas estandarizadas, es decir, las estimaciones de coeficientes que resultan de realizar la regresión de crestas utilizando predictores estandarizados.\n",
        "\n",
        "#### ¿Por qué mejora la regresión de crestas con respecto a los mínimos cuadrados?\n",
        "\n",
        "La ventaja de la regresión de crestas sobre los mínimos cuadrados se basa en la compensación de sesgo-varianza. A medida que aumenta $\\lambda$, la flexibilidad del ajuste de regresión de crestas disminuye, lo que lleva a una menor varianza pero a un mayor sesgo. Esto se ilustra en el panel de la izquierda de la Figura 2, utilizando un conjunto de datos simulados que contiene p = 45 predictores y n = 50 observaciones. La curva verde en el panel de la izquierda de la Figura 2 muestra la varianza de las predicciones de regresión de la cresta como una función de $\\lambda$. En las estimaciones del coeficiente de mínimos cuadrados, que corresponden a la regresión de crestas con $\\lambda = 0$, la varianza es alta pero no hay sesgo. Pero a medida que aumenta $\\lambda$, la contracción de las estimaciones del coeficiente de cresta conduce a una reducción sustancial en la varianza de las predicciones, a expensas de un ligero aumento del sesgo. Recuerde que el error cuadrático medio de la prueba (MSE), representado en púrpura, es una función de la varianza más el sesgo cuadrático. Para valores de $\\lambda$ hasta aproximadamente 10, la varianza disminuye rápidamente, con muy poco aumento en el sesgo, representado en negro. En consecuencia, el MSE cae considerablemente a medida que $\\lambda$ aumenta de 0 a 10. Más allá de este punto, la disminución de la varianza debido al aumento de $\\lambda$ se ralentiza y la contracción de los coeficientes hace que se subestimen significativamente, lo que da como resultado un gran aumento en el sesgo. El MSE mínimo se logra aproximadamente en $\\lambda = 30$. Curiosamente, debido a su alta varianza, el MSE asociado con el ajuste por mínimos cuadrados, cuando $\\lambda = 0$, es casi tan alto como el del modelo nulo para el que todas las estimaciones de coeficientes son cero, cuando $\\lambda = \\infty$. Sin embargo, para un valor intermedio de $\\lambda$, el MSE es considerablemente menor\n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/2c271cb0c8db996449d03b37e1aef9dc.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.2</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "El panel de la derecha de la Figura 2 muestra las mismas curvas que el panel de la izquierda, esta vez graficadas contra la norma $\\ell_2$ de las estimaciones del coeficiente de regresión de la cresta dividida por la norma $\\ell_2$ de las estimaciones de mínimos cuadrados.\n",
        "\n",
        "En general, en situaciones en las que la relación entre la respuesta y los predictores es casi lineal, las estimaciones de mínimos cuadrados tendrán un sesgo bajo pero pueden tener una gran varianza. Esto significa que un pequeño cambio en los datos de entrenamiento puede provocar un gran cambio en las estimaciones del coeficiente de mínimos cuadrados. En particular, cuando el número de variables $p$ es casi tan grande como el número de observaciones $n$, como en el ejemplo de la Figura 2, las estimaciones de mínimos cuadrados serán extremadamente variables. Y si $p > n$, entonces las estimaciones de mínimos cuadrados ni siquiera tienen una solución única, mientras que la regresión de crestas aún puede funcionar bien intercambiando un pequeño aumento en el sesgo por una gran disminución en la varianza. Por lo tanto, la regresión de crestas funciona mejor en situaciones donde las estimaciones de mínimos cuadrados tienen una alta varianza.\n",
        "\n",
        "La regresión de crestas también tiene ventajas computacionales sustanciales sobre la mejor selección de subconjuntos, que requiere la búsqueda a través de modelos $2^p$. Como discutimos anteriormente, incluso para valores moderados de $p$, tal búsqueda puede ser computacionalmente inviable. Por el contrario, para cualquier valor fijo de $\\lambda$, la regresión de crestas solo se ajusta a un único modelo, y el procedimiento de ajuste del modelo se puede realizar con bastante rapidez. De hecho, se puede demostrar que los cálculos necesarios para resolver (figura 2), simultáneamente para todos los valores de $\\lambda$, son casi idénticos a los que se utilizan para ajustar un modelo mediante mínimos cuadrados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zUPIiB5a8vv"
      },
      "source": [
        "### El lasso\n",
        "\n",
        "La regresión de crestas tiene una desventaja obvia. A diferencia de la selección de mejor subconjunto, paso hacia adelante y paso hacia atrás, que generalmente seleccionará modelos que involucran solo un subconjunto de las variables, la regresión de crestas incluirá todos los predictores $p$ en el modelo final. La penalización $\\lambda\\sum\\beta^2_j$ en la regresión de cresta reducirá todos los coeficientes hacia cero, pero no establecerá ninguno de ellos exactamente en cero (a menos que $\\lambda = \\infty$). Esto puede no ser un problema para la precisión de la predicción, pero puede crear un desafío en la interpretación del modelo en entornos en los que el número de variables es bastante grande. Por ejemplo, en el conjunto de datos de **crédito**, parece que las variables más importantes son el **ingreso**, el **límite**, la **calificación** y el **estudiante**, por lo que es posible que deseemos construir un modelo que incluya solo estos predictores. Sin embargo, la regresión de crestas siempre generará un modelo que incluya los diez predictores. Incrementar el valor de $\\lambda$ tenderá a reducir las magnitudes de los coeficientes, pero no resultará en la exclusión de ninguna de las variables.\n",
        "\n",
        "El lasso es una alternativa relativamente reciente a la regresión de la cresta que supera esta desventaja. Los coeficientes de lazo, $\\hat{\\beta}^L_\\lambda$, minimizan la cantidad (eq4)\n",
        "\n",
        "$$ \\sum^n_{i=1} = \\left ( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right )^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j| $$\n",
        "\n",
        "vemos que la regresión de lasso y cresta tienen formulaciones similares. La única diferencia es que el término $\\beta_j^2$ en la penalización por regresión de la cresta ha sido reemplazado por $|\\beta_j|$ en la penalización de lasso. En el lenguaje estadístico, el lasso usa una penalización de $\\ell_1$ en lugar de una penalización de $\\ell_2$. La norma $\\ell_1$ de un vector de coeficientes $\\beta$ viene dada por $||\\beta||_1= \\sum|\\beta_j|$.\n",
        "\n",
        "Al igual que con la regresión de crestas, el lasso reduce las estimaciones de coeficientes hacia cero. Sin embargo, en el caso del lasso, la penalización $\\ell_1$ tiene el efecto de forzar algunas de las estimaciones de coeficientes a ser exactamente iguales a cero cuando el parámetro de ajuste $\\lambda$ es suficientemente grande. Por lo tanto, al igual que la selección del mejor subconjunto, el lasso realiza la selección de variables. Como resultado, los modelos generados a partir del lasso son generalmente mucho más fáciles de interpretar que los producidos por regresión de crestas. Decimos que el lasso produce **modelos escasos**. Como en la regresión de crestas, es fundamental seleccionar un buen valor de $\\lambda$ para el lasso. \n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/60ac794bedc5969a82cf00bcf63eeeac.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.3</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "Como ejemplo, considere las gráficas de coeficientes en la Figura 3, que se generan al aplicar el lasso al conjunto de datos de **Crédito**. Cuando $\\lambda = 0$, entonces el lasso simplemente da el ajuste de mínimos cuadrados, y cuando $\\lambda$ se vuelve lo suficientemente grande, el lasso da el modelo nulo en el que todas las estimaciones de coeficientes son iguales a cero. Sin embargo, entre estos dos extremos, los modelos de regresión de cresta y lasso son bastante diferentes entre sí. Moviéndose de izquierda a derecha en el panel de la derecha de la Figura 3, observamos que al principio el lasso da como resultado un modelo que contiene solo el predictor de calificación. Luego, el alumno y el limitador ingresan al modelo casi simultáneamente, seguido poco después por los ingresos. Finalmente, las variables restantes ingresan al modelo. Por tanto, dependiendo del valor de $\\lambda$, el lasso puede producir un modelo que involucre cualquier número de variables. Por el contrario, la regresión de crestas siempre incluirá todas las variables del modelo, aunque la magnitud de las estimaciones de los coeficientes dependerá de $\\lambda$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVATKzrDa8tb"
      },
      "source": [
        "#### Otra formulación para la regresión de crestas (Ridge Regression) y el lasso\n",
        "\n",
        "Se puede demostrar que las estimaciones del coeficiente de regresión de lasso y cresta resuelven los problemas (eq5, eq6)\n",
        "\n",
        "$$ \\text{minimize}_\\beta \\left \\{ \\sum_{i=1}^n \\left ( y_1 - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 \\right \\} \\quad \\text{sujeo a} \\quad \\sum_{j=1}^p |\\beta_j| \\leq s$$\n",
        "\n",
        "\n",
        "$$ \\text{minimize}_\\beta \\left \\{ \\sum_{i=1}^n \\left ( y_1 - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 \\right \\} \\quad \\text{sujeo a} \\quad \\sum_{j=1}^p \\beta_j^2 \\leq s$$\n",
        "\n",
        "En otras palabras, para cada valor de $\\lambda$, hay algunos tales que las ecuaciones (eq4) y (eq5) darán las mismas estimaciones de coeficientes de lasso. De manera similar, para cada valor de $\\lambda$ hay una correspondencia tal que las ecuaciones (eq2) y (eq6) darán las mismas estimaciones del coeficiente de regresión de cresta. Cuando p = 2, entonces (eq5) indica que las estimaciones del coeficiente de lasso tienen el RSS más pequeño de todos los puntos que se encuentran dentro del diamante definido por $|\\beta_1 | + | \\beta_2 | \\leq s$. De manera similar, las estimaciones de regresión de crestas tienen el RSS más pequeño de todos los puntos que se encuentran dentro del círculo definido por $\\beta_1^2 + \\beta_2^2 \\leq s$\n",
        "\n",
        "Podemos pensar en (eq5) como sigue. Cuando realizamos el lazo, estamos tratando de encontrar el conjunto de estimaciones de coeficientes que conducen a la RSS más pequeña, sujeto a la restricción de que existe una estimación $s$ para el tamaño de $\\sum_{j=1}^p | \\beta_j |$ puede ser. Cuando $s$ es extremadamente grande, entonces esta estimación no es muy restrictiva y, por lo tanto, las estimaciones de coeficientes pueden ser grandes. De hecho, si es lo suficientemente grande como para que la solución de mínimos cuadrados se incluya en la estimación, entonces (eq5) simplemente arrojará la solución de mínimos cuadrados. Por el contrario, si es pequeño, entonces $\\sum_{j=1}^p |\\beta_j|$ debe ser pequeño para no violar la estimación. De manera similar, (eq6) indica que cuando realizamos la regresión de crestas, buscamos un conjunto de estimaciones de coeficientes tales que la RSS sea lo más pequeña posible, sujeto al requisito de que $\\sum_{j=1}^p \\beta_j^2$ no exceda la estimación $s$.\n",
        "\n",
        "#### La propiedad de selección de variable del lasso\n",
        "\n",
        "Las formulaciones (eq5) y (eq6) se pueden utilizar para arrojar luz sobre el tema. La figura 4 ilustra la situación. La solución de mínimos cuadrados se marca como $\\hat{\\beta}$, mientras que el diamante azul y el círculo representan las restricciones de regresión de lasso y cresta en (eq5) y (eq6), respectivamente. Si $s$ es suficientemente grande, entonces las regiones de restricción contendrán $\\hat{\\beta}$, por lo que la regresión de cresta y las estimaciones de lasso serán las mismas que las estimaciones de mínimos cuadrados. Sin embargo, en la Figura 4 las estimaciones de mínimos cuadrados se encuentran fuera del diamante y el círculo, por lo que las estimaciones de mínimos cuadrados no son las mismas que las estimaciones de regresión de lasso y cresta.\n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/7a52b0e919eebb2d4bfc963574ad2084.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.4</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "Las elipses que se centran alrededor de $\\hat{\\beta}$ representan regiones de RSS constante. En otras palabras, todos los puntos de una elipse determinada comparten un valor común del RSS.\n",
        "\n",
        "Las ecuaciones (eq5) y (eq6) indican que las estimaciones del coeficiente de regresión del lasso y la cresta están dadas por el primer punto en el que una elipse contacta con la región de restricción. Dado que la regresión de la cresta tiene una restricción circular sin puntos afilados, esta intersección generalmente no ocurrirá en un eje, por lo que las estimaciones del coeficiente de regresión de la cresta serán exclusivamente distintas de cero. Sin embargo, la restricción de lasso tiene esquinas en cada uno de los ejes, por lo que la elipse a menudo intersecará la región de restricción en un eje. Cuando esto ocurre, uno de los coeficientes será igual a cero. En dimensiones superiores, muchas de las estimaciones de coeficientes pueden ser iguales a cero simultáneamente. En la figura 4, la intersección ocurre en $\\beta_1 = 0$, por lo que el modelo resultante solo incluirá $\\beta_2$.\n",
        "\n",
        "En la Figura 4, consideramos el caso simple de p = 2. Cuando p = 3, la región de restricción para la regresión de cresta se convierte en una esfera y la región de restricción para el lasso se convierte en un poliedro. Cuando p > 3, la restricción para la regresión de la cresta se convierte en una hiperesfera y la restricción para el lasso se convierte en un politopo. Sin embargo, las ideas clave que se muestran en la Figura 4 aún se mantienen. En particular, el lasso conduce a la selección de características cuando p > 2 debido a las esquinas afiladas del poliedro o politopo.\n",
        "\n",
        "#### Comparación de la regresión de lasso y cresta (ridge)\n",
        "\n",
        "Está claro que el lasso tiene una gran ventaja sobre la regresión de crestas, ya que produce modelos más simples e interpretables que involucran solo a un subconjunto de predictores.\n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/d0ab6080feeca44e3154735289ac6cf0.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.5</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "La Figura 5 muestra la varianza, el sesgo al cuadrado y el MSE de prueba del lasso aplicado a los mismos datos simulados que en la Figura 2. Claramente, el lasso conduce a un comportamiento cualitativamente similar a la regresión de la cresta, en el sentido de que a medida que aumenta $\\lambda$, la varianza disminuye y el sesgo aumenta. En el panel de la derecha de la Figura 5, las líneas de puntos representan los ajustes de regresión de la cresta. Aquí graficamos ambos contra su $R^2$ en los datos de entrenamiento. Esta es otra forma útil de indexar modelos y se puede utilizar para comparar modelos con diferentes tipos de regularización, como es el caso aquí. En este ejemplo, la regresión de lasso y cresta dan como resultado sesgos casi idénticos. Sin embargo, la varianza de la regresión de la cresta es ligeramente menor que la varianza del lasso, por lo que el MSE mínimo de la regresión de la cresta es ligeramente más pequeño que el del lasso.\n",
        "\n",
        "Sin embargo, los datos de la Figura 5 se generaron de tal manera que los 45 predictores se relacionaron con la respuesta, es decir, ninguno de los coeficientes verdaderos $\\beta_1, ..., \\beta_45$ fue igual a cero. El lasso asume implícitamente que un número de coeficientes realmente es igual a cero. En consecuencia, no es sorprendente que la regresión de la cresta supere al lasso en términos de error de predicción en este entorno. La figura 6 ilustra una situación similar, excepto que ahora la respuesta es una función de solo 2 de los 45 predictores. Ahora, el lasso tiende a superar la regresión de la cresta en términos de sesgo, varianza y MSE\n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/2838afe3228a15e68ac77cbb6ac0ec74.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.6</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "Estos dos ejemplos ilustran que ni la regresión de la cresta ni el lasso dominarán universalmente al otro. En general, se podría esperar que el lasso funcione mejor en un entorno donde un número relativamente pequeño de predictores tiene coeficientes sustanciales y los predictores restantes tienen coeficientes que son muy pequeños o iguales a cero. La regresión de crestas funcionará mejor cuando la respuesta sea una función de muchos predictores, todos con coeficientes de aproximadamente el mismo tamaño. Sin embargo, el número de predictores relacionados con la respuesta nunca se conoce a priori para conjuntos de datos reales. Se puede utilizar una técnica como la validación cruzada para determinar qué enfoque es mejor en un conjunto de datos en particular.\n",
        "\n",
        "Al igual que con la regresión de crestas, cuando las estimaciones de mínimos cuadrados tienen una varianza excesivamente alta, la solución de lasso puede producir una reducción en la varianza a expensas de un pequeño aumento en el sesgo y, en consecuencia, puede generar predicciones más precisas. A diferencia de la regresión de crestas, el lasso realiza una selección de variables y, por lo tanto, da como resultado modelos que son más fáciles de interpretar.\n",
        "\n",
        "#### Un caso especial simple para la regresión de crestas y el lasso\n",
        "\n",
        "Para obtener una mejor intuición sobre el comportamiento de la regresión de la cresta y el lasso, considere un caso especial simple con $n = p$, y $X$ una matriz diagonal con unos en la diagonal y ceros en todos los elementos fuera de la diagonal. Para simplificar aún más el problema, suponga también que estamos realizando una regresión sin una intersección. Con estos supuestos, el problema habitual de mínimos cuadrados se simplifica para encontrar $\\beta_1, ..., \\beta_p$ que minimizan: (eq7)\n",
        "\n",
        "$$ \\sum_{j=1}^p (y_i - \\beta_j)^2$$\n",
        "\n",
        "En este caso, la solución de mínimos cuadrados viene dada por\n",
        "\n",
        "$$ \\hat{\\beta}_j = y_j $$\n",
        "\n",
        "Y en este escenario, la regresión de la cresta equivale a encontrar $\\beta_1, ..., \\beta_p$ tal que: (eq8)\n",
        "\n",
        "$$ \\sum_{j=1}^p (y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 $$\n",
        "\n",
        "se minimiza, y el lasso equivale a encontrar los coeficientes tales que (eq9)\n",
        "\n",
        "$$ \\sum_{j=1}^p (y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| $$\n",
        "\n",
        "\n",
        "se minimiza. Se puede mostrar que en este escenario, las estimaciones de regresión de crestas toman la forma (eq10)\n",
        "\n",
        "$$ \\hat{\\beta}^R_j = \\frac{y_j}{1 + \\lambda}$$\n",
        "\n",
        "y las estimaciones de lasso toman la forma (eq11)\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}^L_j = \\left\\{ \\begin{array}{lcc}\n",
        "             y_j - \\lambda/2 &   si  & y_j > \\lambda/2 \\\\\n",
        "             \\\\ y_j + \\lambda/2 &  si & y_j < -\\lambda/2 \\\\\n",
        "             \\\\ 0 &  si  & |y_j| \\leq \\lambda/2\n",
        "             \\end{array}\n",
        "   \\right.\n",
        "$$\n",
        "\n",
        "La figura 7 muestra la situación. Podemos ver que la regresión de la cresta y el lasso realizan dos tipos de contracción muy diferentes. En la regresión de crestas, cada estimación del coeficiente de mínimos cuadrados se reduce en la misma proporción. En contraste, el lasso encoge cada coeficiente de mínimos cuadrados hacia cero en una cantidad constante, $\\lambda / 2$; los coeficientes de mínimos cuadrados que son menores que $\\lambda / 2$ en valor absoluto se reducen completamente a cero. El tipo de contracción que realiza el lasso en esta configuración simple (eq11) se conoce como **umbral suave** (soft-thresholding). El hecho de que algunos coeficientes de lasso se reduzcan por completo a cero explica por qué el lasso realiza la selección de funciones.\n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/705c6a9e9d1a9272299ca77e77324f0e.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.7</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "En el caso de una matriz de datos más general $X$, la historia es un poco más complicada que la que se muestra en la Figura 7, pero las ideas principales aún se mantienen aproximadamente: la regresión de crestas reduce más o menos cada dimensión de los datos en la misma proporción, mientras que el lasso reduce más o menos todos los coeficientes hacia cero en una cantidad similar, y los coeficientes suficientemente pequeños se reducen hasta cero.\n",
        "\n",
        "#### Interpretación bayesiana para la regresión de crestas y el lasso\n",
        "\n",
        "Ahora mostramos que se puede ver la regresión de la cresta y el lasso a través de una vista bayesiana. Un punto de vista bayesiano para la regresión supone que el vector de coeficientes $\\beta$ tiene alguna distribución previa, digamos $p (\\beta)$, donde$ \\beta = (\\beta_0, \\beta_1, ..., \\beta_p)^T$. La probabilidad de los datos se puede escribir como $f (Y | X, \\beta)$, donde $X = (X_1, ..., X_p)$. Multiplicar la distribución anterior por la probabilidad nos da (hasta una constante de proporcionalidad) la distribución posterior, que toma la forma:\n",
        "\n",
        "$$ p(\\beta|X,Y) \\quad \\alpha \\quad f(Y|X, \\beta)p(\\beta|X) = f(Y|X, \\beta)p(\\beta)$$\n",
        "\n",
        "donde la proporcionalidad anterior se sigue del teorema de Bayes, y la igualdad anterior se deriva del supuesto de que $X$ es fijo. Suponemos el modelo lineal habitual,\n",
        "\n",
        "$$ Y = \\beta_0 + X_1\\beta_1 + ... + X_p\\beta_p + \\epsilon$$\n",
        "\n",
        "y suponga que los errores son independientes y se extraen de una distribución normal. Además, suponga que $p(\\beta) = \\prod^p_{j=1} g(\\beta_j)$, para alguna función de densidad $g$. Resulta que la regresión de la cresta y el lasso se siguen naturalmente de dos casos especiales de $g$:\n",
        "\n",
        "- Si $g$ es una distribución gaussiana con media cero y desviación estándar una función de $\\lambda$, entonces se deduce que el modo posterior de $\\beta$, es decir, el valor más probable de $\\beta$, dados los datos, viene dado por la solución de regresión de cresta. (De hecho, la solución de regresión de la cresta también es la media posterior). \n",
        "- Si $g$ es una distribución doble exponencial (Laplace) con media cero y el parámetro de escala una función de $\\lambda$, entonces se deduce que el modo posterior para $\\beta$ es la solución de lasso. (Sin embargo, la solución de lasso no es la media posterior y, de hecho, la media posterior no produce un vector de coeficiente disperso).\n",
        "\n",
        "<center>\n",
        "<p>\n",
        "<img src=\"https://i.gyazo.com/4ca2535fa64f6fa93f363676701146c2.png\" width=\"80%\"/>\n",
        "<figcaption>Fig.8</figcaption>\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "El gaussiana y distribuciones previas de doble exponencial se muestran en la Figura 8. Por lo tanto, desde un punto de vista Bayesiano, de regresión cresta y el lasaso se siguen directamente de asumir el modelo lineal usual con errores normales, junto con una simple distribución previa para $\\beta$. Observe que el lasso anterior tiene un pico abrupto en cero, mientras que el gaussiano es más plano y más grueso en cero. Por tanto, el lasso espera a priori que muchos de los coeficientes sean (exactamente) cero, mientras que el puente supone que los coeficientes están distribuidos aleatoriamente alrededor de cero.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53MgC654a8q-"
      },
      "source": [
        "### Selección del parámetro de afinación "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv-peoeRa8oy"
      },
      "source": [
        "## Métodos de reducción de dimensiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11-JpSQJcciR"
      },
      "source": [
        "### Componentes Principales de regresión\n",
        "\n",
        "#### Una descripción general del análisis de componentes principales\n",
        "\n",
        "#### El enfoque a los componentes principales de regresión\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5lTvRj_ccn4"
      },
      "source": [
        "### Mínimos cuadrados parciales\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCu8XZpedXU-"
      },
      "source": [
        "### Consideraciones en grandes dimensiones \n",
        "\n",
        "#### Datos de alta dimensión\n",
        "\n",
        "#### ¿Qué sale mal en las dimensiones altas?\n",
        "\n",
        "#### Regresión en altas dimensiones\n",
        "\n",
        "#### Interpretación de resultados en grandes dimensiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viEOxPLNdXcA"
      },
      "source": [
        "## Lab: Regresión de crestas y lazo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qBxEhwDdXYt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVQkiRL0dXMJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv0JUpGWY3JN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}