{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tema 7 Estadistica_r.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidCarneros/notebooks_uned/blob/main/Tema_7_Estadistica_r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVf2KQZZXgLQ"
      },
      "source": [
        "# Análisis discriminante lineal\r\n",
        "\r\n",
        "El Análisis Discriminante Lineal (LDA, por sus siglas en inglés) es un método de clasificación supervisado orientado a problemas en los que la variable dependiente es categórica con más de dos clases. Haciendo uso del teorema de Bayes, LDA estima la probabilidad de que una observación, dado un determinado valor en los predictores, pertenezca a cada una de las clases de la variable respuesta: $Pr(Y = k | X = x)$. Finalmente, se asigna la observación a la clase $k$ para la que la probabilidad predicha es mayor.\r\n",
        "\r\n",
        "LDA es una alternativa a la regresión logística (tema 8), para el caso en el que la variable respuesta puede tomar más de dos valores. Aunque existen extensiones de la regresión logística para problemas de clasificación multiclase, el LDA presenta una serie de ventajas:\r\n",
        "\r\n",
        "- Si las clases están bien separadas, los parámetros estimados en el modelo de regresión logística son inestables. El método de LDA no sufre este problema.\r\n",
        "\r\n",
        "- Si el número de observaciones $n$ es pequeño, y los predictores siguen una distribución aproximadamente normal en cada una de las clases, LDA también es más estable que la regresión logística.\r\n",
        "\r\n",
        "No obstante, en problemas de clasificación binarios, ambos métodos suelen llegar a resultados similares. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMGvv3Z_bUnN"
      },
      "source": [
        "## Teorema de Bayes para clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pZVbFt-es67"
      },
      "source": [
        "El teorema de Bayes es útil para obtener probabilidades condicionales, ya que expresa la probabilidad condicional de un evento aleatorio A dado que otro evento B ya ha ocurrido. Esta probabilidad es igual a la probabilidad conjunta de A y B, $P(AB)$ entre la probabilidad marginal de B, $P(B)$:\r\n",
        "\r\n",
        "$$ P(A_j|B) = \\frac{P(A_jB)}{P(B)} = \\frac{P(A_j) \\cdot P(B|A_j)}{\\sum ^n _{i=1} P(A_i) \\cdot P(B|A_j)}$$\r\n",
        "\r\n",
        "\r\n",
        "En el contexto de la clasificación, mediante el teorema de Bayes se calcula la probabilidad de que la variable respuesta Y pertenezca a cada uno de los posibles niveles, dados unos determinados valores de los predictores. Supongamos que queremos clasificar una observación en una de $K$ clases, tal que $K \\geq 2$. Hay que considerar:\r\n",
        "\r\n",
        "- La **probabilidad a priori** (*prior probability*) de que un individuo tomado al azar pertenezca a la clase k-ésima, $\\pi _k$. En general, estimar $\\pi _k$ es fácil si tenemos una muestra aleatoria de valores de $Y$: solo tenemos que calcular la proporción de observaciones de entrenamiento que pertenecen a la clase k-ésima.\r\n",
        "\r\n",
        "$$ \\hat \\pi _k = \\frac{n_k}{n}$$\r\n",
        "\r\n",
        "- La **función de densidad de $X$** para una observación que pertenece a la clase k-ésima, definida como $f_k(x) = P(X = x | Y = k)$. Cuanto mayor sea $f_k(x)$, mayor será la probabilidad de que una observación de la clase k-ésima tome un valor $X \\approx x$. También deberemos estimar $f_k (x)$ a través de la muestra, ya que raramente disponemos los parámetros poblacionales. Esto hace que el clasificador LDA suponga una aproximación al clasificador de Bayes.\r\n",
        "\r\n",
        "El teorema de Bayes establece que:\r\n",
        "\r\n",
        "$$Pr(Y=k | X = x) = \\frac{\\pi _k f_k(x)}{\\sum ^K _{l=1} \\pi _l f_l(x)}$$\r\n",
        "\r\n",
        "\r\n",
        "Donde $p_k(x)$ o $Pr(Y=k | X = x)$ representa la  **probabilidad a posteri** (*posterior probability*), que es la probabilidad de que una observación $X=x$ pertenezca a la clase $k$.  Es decir, la probabilidad de la variable respuesta dado el valor del predictor (probabilidad condicional).  Cada observación se clasificará dentro del nivel que tiene la probabilidad $p_k(x)$ más alta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5EKppK4bcMk"
      },
      "source": [
        "## Análisis discriminante lineal para $p = 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5780qGLbcTb"
      },
      "source": [
        "A la hora de estimar $f_k(x)$ en un escenario con un solo predictor ($p=1$), deberemos contar con ciertas suposiciones sobre su forma. Si asumimos que $f_k(x)$ sigue una distribución normal, en un escenario unidimensional, la función de densidad normal tomaría la forma:\r\n",
        "\r\n",
        "$$f_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma _k}}\\text{exp}(-\\frac{1}{2\\sigma ^2_k}(x-\\mu _k)^2)$$\r\n",
        "\r\n",
        "Donde $\\mu _k$ es la media y $\\sigma ^2 _k$ es la varianza para la clase $k$. Por ahora, supongamos que $\\sigma ^2 _1 = ... = \\sigma ^2 _k$, es decir, todas las clases tienen la misma varianza, que para simplificar, podemos denotar como $\\sigma ^2$.  Incluyendo la fórmula anterior de $f_k(x)$ en la ecuación de Bayes, obtenemos:\r\n",
        "\r\n",
        "$$p_k(x) = \\frac{\\pi _k \\frac{1}{\\sqrt{2\\pi\\sigma}}\\text{exp}(-\\frac{1}{2\\sigma ^2}(x-\\mu _k)^2)}{\\sum ^K _{l=1} \\pi _l \\frac{1}{\\sqrt{2\\pi\\sigma}}\\text{exp}(-\\frac{1}{2\\sigma ^2}(x-\\mu _l)^2)}$$\r\n",
        "\r\n",
        "Mediante la transformación logarítmica de los términos de la ecuación anterior, y reagrupando los términos, podemos obtener una ecuación más simplificada:\r\n",
        "$$\\delta _k(x) = x \\cdot \\frac{\\mu _k}{\\sigma ^2} - \\frac{\\mu ^2 _k}{2 \\sigma ^2} + log(\\pi _k)$$\r\n",
        "\r\n",
        "donde $\\delta _k(x)$ es el log de $p_k (x)$.  Con esta última ecuación, el clasificador LDA asigna una observación $X = x$ a la clase que maximiza $\\delta _k(x)$. La función discriminante $\\delta _k(x)$  es lineal respecto de x, de ahí el término *lineal* en el nombre de LDA, y por ello, generará **límites de decisión lineales**. \r\n",
        "\r\n",
        "### Límites de decisión\r\n",
        "\r\n",
        "Supongamos que K = 2 y que la probabilidad a priori $\\pi _k$ es 0.5, es decir $\\pi _1 = \\pi _2$, el límite de decisión de Bayes vendría dado por:\r\n",
        "\r\n",
        "$$x = \\frac{\\mu ^2 _1 - \\mu ^2 _2}{2(\\mu _1 - \\mu _2)} = \\frac{\\mu _1 + \\mu _2}{2}$$\r\n",
        "\r\n",
        "Un ejemplo se muestra en el panel izquierdo de la Figura 4.4. Las dos funciones de densidad normales que se representan gráficamente, $f _1(x)$ y $f _2(x)$, se corresponden con dos clases distintas, con media $\\mu _1 = - 1,25$ y varianza $\\sigma ^2 _1 = 1$, y por otro lado, media $\\mu _2 = 1,25$ y varianza $\\sigma ^2 _2 = 1$. En este caso, el clasificador Bayes asignará la observación a la clase k=1 (verde) si $x \\lt 0$, y a la clase k=2 (rojo) si $x \\gt 0$.\r\n",
        "\r\n",
        "Sin embargo, en la práctica no conocemos los parámetros poblacionales, aunque tengamos cierta seguridad de que X se distribuye de forma normal para cada clase. Por ello, no podemos calcular el límite de decisión de Bayes. En este caso, debemos obtener las estimaciones de los parámetros  $\\mu _1= ... = \\mu _k$, $\\sigma ^2$, y $\\pi _1=... = \\pi _k$ a partir de la muestra. En concreto, el análisis discrminante lineal se basa en las siguientes estimaciones:\r\n",
        "\r\n",
        "$$\\hat \\mu _k = \\frac{1}{n_k} \\sum _{i:y_i=k} x _i$$\r\n",
        "\r\n",
        "$$\\hat \\sigma ^2 = \\frac{1}{n - K} \\sum _{k=1} ^K \\sum _{i:y_i=k} (x_i - \\hat \\mu _k)^2$$\r\n",
        "\r\n",
        "Donde $n$ es el número total de observaciones de entrenamiento, y $n _k$ es el número de observaciones de entrenamiento para la clase $k$. La estimación de $\\mu _k$ es simplemente la media de todas las observaciones para la clase $k$. Como ya hemos comentado, LDA estima $\\pi _k$ como la proporción de instancias de entrenamiento que pertenecen a la clase k-ésima. \r\n",
        "\r\n",
        "El clasificador LDA utiliza estas estimaciones para asignar la observación $X = x$ a la clase para la que \r\n",
        "\r\n",
        "$$\\hat \\delta _k(x) = x \\cdot \\frac{\\hat \\mu _k}{\\hat \\sigma ^2} - \\frac{\\hat \\mu ^2 _k}{2 \\hat \\sigma ^2} + log(\\hat \\pi _k)$$\r\n",
        "\r\n",
        "es mayor.\r\n",
        "\r\n",
        "El panel derecho de la Figura 4.4 muestra un histograma para una muestra de 20 observaciones para cada clase ($n_1 = n_2 = 20$). Para implementar LDA, empezamos estimado los parámetros $\\pi _k$, $\\mu _k$ y $\\sigma ^2$. \r\n",
        "\r\n",
        "En este caso, dado que $n_1=n2$, tenemos $\\pi _1= \\pi _2$, con lo que en esta situación el límite de decisión corresponderá al punto medio de la media de las dos clases. En la figura podemos ver que el límite de decisión LDA está ligeramente desplazado hacia la derecha en relación al límite de decisión de Bayes, pero sigue siendo una aproximación bastante buena.\r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/cfeed1589da51617c6f9f5b6c278f31b.png\" width=\"80%\"/>\r\n",
        "<figcaption>Figura 4.4. Izquierda: Se muestran dos funciones de densidad bidimensionales. Las líneas de puntos representan el límite de decisión de Bayes. En la parte derecha, se dibujan 20 observaciones de cada clase como histogramas. </figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IG6Oc58bkx8"
      },
      "source": [
        "## Análisis discriminante lineal para $p \\gt 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9tBu_NoGY-n"
      },
      "source": [
        "  Podemos extender el LDA para casos con más de un predictor. En este caso ha de asumirse que $X = (X_1, X_2,...,X_p)$ siguen una **distribución normal multivariante**, con media $\\mu _k$ específica de la clase y varianza $\\sigma ^2$ común entre clases. \r\n",
        "\r\n",
        "\r\n",
        "  ### Distribución normal multivariante\r\n",
        "\r\n",
        "La distribución normal multivariante asume que cada uno de los predictores se distribuyen de forma normal, con una cierta correlación entre cada par de predictores. Las imágenes de la Figura 4.5 muestran representaciones gráficas de distribuciones normales multivariante de 2 elementos (distribución normal bivariante).\r\n",
        "\r\n",
        "La altura de la superficie en un punto concreto representa la probabilidad de que tanto $X_1$ como $X_2$ caigan en una pequeña región alrededor del punto. En el ejemplo de la izquierda se observa una forma de campana debido a que $Var(X_1) = Var(X_2)$ y $Cor(X_1, X_2) = 0$. Sin embargo, esta forma de campana se distorsiona si los predictores están correlacionados o no tienen igualdad de varianzas. Esto se ilustra en el panel derecho de la Figura 4.5. \r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/50a896b2463f6e0f5f3c1489afd75e79.png\" width=\"80%\"/>\r\n",
        "<figcaption>Figura 4.5. Dos funciones de densidad con $p=2$. En la izquierda, los predictores no están correlacionados. En la derecha, los predictores tienen una correlación de 0.7  </figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n",
        "Se utiliza el término $X \\sim N (\\mu, \\sum)$ para indicar que una variable aleatoria $X$ p-dimensional sigue una distribución normal multivariante, donde $\\mu$ es su media y donde $\\sum$ es su matriz de covarianza $p \\times p$ común a todas las clases $L$. La ecuación se define como:\r\n",
        "\r\n",
        "$$f(x) = \\frac{1}{(2\\pi)^{p/2}|\\sum|^{1/2}} \\ exp \\ (-\\frac{1}{2}(x-\\mu)^T\\sum\\ ^{-1} \\ (x-\\mu))$$\r\n",
        "\r\n",
        " Introduciendo $f(x)$ en la ecuación de Bayes, obtenemos un clasificador que asignará la observación $X = x$ a la clase para la que se maximice la siguiente ecuación:\r\n",
        "\r\n",
        "$${\\delta}_k(x) = x^T \\ \\sum \\ ^{-1} \\  \\mu_k -\\ \\frac{1}{2} \\mu_k^T\\sum \\ ^{-1} \\ \\mu_k + log(\\pi_k)$$\r\n",
        "\r\n",
        "\r\n",
        "Se muestra un ejemplo en el panel izquierdo de la Figura 4.6 con tres clases Gaussianas con variazan común. Las tres elipses representan las regiones que contienen el 95% de la probabilidad para cada una de las tres clases. Las líneas de puntos son los límites de decisión. En otras palabras, representan el conjunto de valores de $x$ para los cuales $\\delta _k (x) = \\delta _l(x)$. Es decir,\r\n",
        "\r\n",
        "$$ x^T \\ \\sum \\ ^{-1} \\  \\mu_k -\\ \\frac{1}{2} \\mu_k^T\\sum \\ ^{-1} \\ \\mu_k =  x^T \\ \\sum \\ ^{-1} \\  \\mu_l -\\ \\frac{1}{2} \\mu_l^T\\sum \\ ^{-1} \\ \\mu_l $$\r\n",
        "\r\n",
        "para $k \\neq l$. En este caso, el término logarítmico ha desaparecido porque las tres clases tienen el mismo número de observaciones. Por tanto, $\\pi _k$ es igual para las tres clases. \r\n",
        "\r\n",
        "De nuevo, desconociendo los parámetros poblacionales no podemos obtener el límite de decisión de Bayes, por lo que tendremos que estimar $\\mu _1=...=\\mu _k$, $\\pi _1=...=\\pi _k$ y $\\sum$ para obtener los límites de decisión de LDA, que dividen el espacio del predictor en regiones correspondiente a las clases. \r\n",
        "\r\n",
        "En el panel derecho de la Figura 4-6, se representan 20 observaciones para cada una de las clases, y los límites de decisión correspondientes. Nuevamente, los límites de decisión de LDA son muy cercanos a los de Bayes. \r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/0ee9361291212d8f5c06d81c1f38adcc.png\" width=\"80%\"/>\r\n",
        "<figcaption>Figura 4.6.  </figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xtF5Y96_gZY"
      },
      "source": [
        "### Precisión del modelo LDA\r\n",
        "\r\n",
        "El método LDA trata de aproximarse al clasificador de Bayes, el cual comete el menor ratio de error de todos los clasificadores, siempre que se cumpla la condición de normalidad.\r\n",
        "\r\n",
        "Es conveniente obtener la **matriz de confusión** para detectar la cantidad de verdaderos positivos (sensibilidad), verdaderos negativos, falsos positivos (error tipo I) y falsos negativos del clasificador LDA, y determinar el error que comete no solo de forma general, sino en cada dirección de predicción (el modelo puede ser mejor o peor prediciendo en una dirección que en otra).\r\n",
        "\r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/b0a6c902e53bf0df43a2f84f08fde674.png\" width=\"45%\"/>\r\n",
        "<figcaption>Matriz de confusión</figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n",
        "\r\n",
        "Si nos interesa cometer menos predicciones incorrectas, podemos considerar disminuir el límite probabilidad a posteriori para el límite de decisión. Por ejemplo, asignar una observación $x$ a la clase $k_i$ si $Pr(K = k_i │ X = x) \\gt 0,3$, en lugar de $Pr(K = k_i │ X = x) \\gt 0,5$ (límite por defecto). Todo dependerá de qué ratio de error nos interese más, o el coste asociado a cada uno. Es lo que se conoce como **sensibilidad** y **especificidad**. En la Tabla 4.6 se muestra un resumen de los posibles valores que podemos obtener. Por su parte, la tabla 4.7 muestra un resumen de las métricas que se suelen utilizar en la clasificación.\r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/b4d9a91175496b3756504a2cc6ec6bdb.png\" width=\"65%\"/>\r\n",
        "<figcaption>Tabla 4.6.</figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/8f3cd877d9453ac17515bbd89f47586a.png\" width=\"65%\"/>\r\n",
        "<figcaption>Tabla 4.7.</figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Como ejemplo, en la Figura 4.7 se muestra como varía la tasa de error cuando se modifica el límite de la probabildiad a posterir en el dataset `default`. \r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/486b127acdfd608669a0eaaa486990fd.png\" width=\"65%\"/>\r\n",
        "<figcaption>Figura 4.7. Para el dataset default, la tasa de error se presenta como una función del límite de la probabilidad a posteri que se utiliza para realizar la clasificación</figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n",
        "\r\n",
        "La **curva de ROC** es un gráfico útil para representar los dos tipos de errores del modelo (global y por clasificación) para todos los posibles thresholds. La Figura 4.8 muestra la curva ROC para un LDA sobre el conjunto de entrenamiento. Una curva de ROC ideal mostraría que para un alto ratio de verdaderos positivos le correspondería un bajo ratio de falsos positivos (cubrirá la esquina superior izquierda). \r\n",
        "\r\n",
        "\r\n",
        "El rendimiento global del modelo sobre todos los posibles thresholds se corresponde con el **área bajo la curva (AUC)**. Cuanto mayor es AUC, mejor es el clasificador. Un clasificador con AUC = 0,5 no superaría lo esperado por azar (evaluado con datos de test). En este ejemplo, el AUC es 0.95, por lo que es un clasificador muy bueno. \r\n",
        "\r\n",
        "Las curvas ROC son útiles para comparar diferentes clasificadores. Para no obtener un porcentaje de error que subestime el verdadero porcentaje de error que puede cometer el modelo, es importante evaluarlo sobre un grupo de datos de test, y no utilizar los datos de entrenamiento que se han utilizado para entrenar el modelo. \r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/631f76f689a9084920ee56ab4f9dbd4d.png\" width=\"55%\"/>\r\n",
        "<figcaption>Figura 4.8. Curva ROC para el clasificador LDA sobre el dataset default</figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCY1sWyabyxc"
      },
      "source": [
        "## Lab: Análisis discriminante lineal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWECI3kvGrfh"
      },
      "source": [
        "En este ejemplo trabajaremos con el dataset `Smarket` del paquete ISLR, que contiene contiene los rendimientos del índice bursátil S&P500 para los años comprendidos entre 2001 y 2005. Para cada fecha, se recoge el porcentaje de los cinco días anteriores (predictores `Lag1` a `Lag5`), el volumen negociado (`Volume`) en billones, `Today`, el valor porcentual para el día en cuestión, y `Direction`, una etiqueta de texto que determina si el mercado sube (*Up*) o baja (*Down*) ese día."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7otLY89Pb2qL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "40d0603f-38d0-4fb5-e7a4-f9432550f678"
      },
      "source": [
        "install.packages('ISLR', repos='http://cran.rstudio.com/')\r\n",
        "library (ISLR)\r\n",
        "names(Smarket)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n",
              "[7] \"Volume\"    \"Today\"     \"Direction\""
            ],
            "text/latex": "\\begin{enumerate*}\n\\item 'Year'\n\\item 'Lag1'\n\\item 'Lag2'\n\\item 'Lag3'\n\\item 'Lag4'\n\\item 'Lag5'\n\\item 'Volume'\n\\item 'Today'\n\\item 'Direction'\n\\end{enumerate*}\n",
            "text/markdown": "1. 'Year'\n2. 'Lag1'\n3. 'Lag2'\n4. 'Lag3'\n5. 'Lag4'\n6. 'Lag5'\n7. 'Volume'\n8. 'Today'\n9. 'Direction'\n\n\n",
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'Year'</li><li>'Lag1'</li><li>'Lag2'</li><li>'Lag3'</li><li>'Lag4'</li><li>'Lag5'</li><li>'Volume'</li><li>'Today'</li><li>'Direction'</li></ol>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BsYLwLeFwur"
      },
      "source": [
        "attach (Smarket)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvt9qZs9IS_2"
      },
      "source": [
        "Vamos a definir un vector booleano `train` con 1.250 elementos correspondientes a las observaciones del dataset. Aquellas que ocurrieron antes de 2005 las vamosa  poner a TRUE, mientras que las que se corresponden con fechas posteriores las pondremos a FALSE. Esto lo utilizaremos para poder entrenar y e valuar nuestro modelo en dos conjuntos de datos diferentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdU8izUTGW0q"
      },
      "source": [
        "train =(Year <2005)\r\n",
        "Smarket.2005= Smarket [!train ,]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG02RMX3MpuT"
      },
      "source": [
        "En R, podemos ajustar un modelo LDA utilizando la función `lda()`, incluida en la librería MASS. La sintáxis es idéntica a la función `lm()`. Vamos a entrenar el modelo utilizando solo las observaciones anteriores a 2005."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "DW82QjZXFnUT",
        "outputId": "974f87b4-9bf2-41f9-f2ae-e2309ffabebf"
      },
      "source": [
        "library (MASS)\r\n",
        "lda.fit=lda(Direction~Lag1+Lag2 ,data=Smarket ,subset =train)\r\n",
        "lda.fit"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Call:\n",
              "lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n",
              "\n",
              "Prior probabilities of groups:\n",
              "    Down       Up \n",
              "0.491984 0.508016 \n",
              "\n",
              "Group means:\n",
              "            Lag1        Lag2\n",
              "Down  0.04279022  0.03389409\n",
              "Up   -0.03954635 -0.03132544\n",
              "\n",
              "Coefficients of linear discriminants:\n",
              "            LD1\n",
              "Lag1 -0.6420190\n",
              "Lag2 -0.5135293"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPEfy0EDM-Py"
      },
      "source": [
        "La salida anterior indica que $\\hat \\pi_1 = 0.492$ y que $\\hat \\pi_2 = 0.508$; o, en otras palabras, que el $49.2\\%$ de las observaciones de entrenamiento se corresponden a días en los que el mercado bajó (`Down`). También proporciona la media de cada grupo; es decir, el valor promedio de cada predictor para cada una de las clases, que se utilizan por LDA para estimar $\\mu _k$. Esto sugiere que hay una tendencia para los dos días previos: es negativo para los días en los que el mercado sube y positivo para los días en los que baja. \r\n",
        "\r\n",
        "Los coeficientes del LDA proporcionan una combinación lineal de `Lag1` y `Lag2` que se utilizan para determinar la regla de decisión. En otras palabras, son los multiplicadores de los elementos de $X = x$ en la ecuación:\r\n",
        "\r\n",
        "$${\\delta}_k(x) = x^T \\ \\sum \\ ^{-1} \\  \\mu_k -\\ \\frac{1}{2} \\mu_k^T\\sum \\ ^{-1} \\ \\mu_k + log(\\pi_k)$$\r\n",
        "\r\n",
        "Si $−0.642 \\times Lag1−0.514 \\times Lag2$ es grande, entonces LDA predecirá un incremento del mercado, mientras que en caso contrario predecirá un descenso. \r\n",
        "\r\n",
        "La función `predict()` devuelve una lista con tres elementos. `class`, que contiene las prediciones de LDA sobre el movimiento del mercado; `posterior`, un matriz en la que la columna k-ésima contiene la probabilidad posterior de que la observación correspondiente pertenezca a la clase k; y `x`, que contiene los discriminantes líneales. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "iqQ40oQlPAC8",
        "outputId": "2fdbee68-6f85-40ce-ce32-94ec3450a996"
      },
      "source": [
        "lda.pred=predict (lda.fit , Smarket.2005)\r\n",
        "names(lda.pred)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] \"class\"     \"posterior\" \"x\"        "
            ],
            "text/latex": "\\begin{enumerate*}\n\\item 'class'\n\\item 'posterior'\n\\item 'x'\n\\end{enumerate*}\n",
            "text/markdown": "1. 'class'\n2. 'posterior'\n3. 'x'\n\n\n",
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'class'</li><li>'posterior'</li><li>'x'</li></ol>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "8sQMR8uGPhfc",
        "outputId": "9aee8546-7b55-44da-c813-c02c1e81f8c0"
      },
      "source": [
        "Direction.2005= Direction [!train]\r\n",
        "lda.class =lda.pred$class\r\n",
        "table(lda.class, Direction.2005)\r\n",
        "mean(lda.pred== Direction.2005)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         Direction.2005\n",
              "lda.class Down  Up\n",
              "     Down   35  35\n",
              "     Up     76 106"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] 0"
            ],
            "text/latex": "0",
            "text/markdown": "0",
            "text/html": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4JOGkCqpH4X"
      },
      "source": [
        "Aplicando un threshold del 50% a la probabilidad posterior, podemos recrear las predicciones contenidas en `lda.pred$class`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "08JwAflJPhih",
        "outputId": "dcf8558d-f06e-4605-aa7a-839adc19e57b"
      },
      "source": [
        "sum(lda.pred$posterior [ ,1] >=.5)\r\n",
        "\r\n",
        "sum(lda.pred$posterior [,1]<.5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] 70"
            ],
            "text/latex": "70",
            "text/markdown": "70",
            "text/html": [
              "70"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] 182"
            ],
            "text/latex": "182",
            "text/markdown": "182",
            "text/html": [
              "182"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGJZTi9gpZbT"
      },
      "source": [
        "Hay que tener en cuenta que la probabilidad posterior devuelta por el modelo se correponde con la probabilidad de que el mercado baje (`Down`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "zFyRUbToPhli",
        "outputId": "bd9c67a7-e5fc-42a2-d20f-319e95cccb7e"
      },
      "source": [
        "lda.pred$posterior [1:20 ,1]\r\n",
        "\r\n",
        "lda.class [1:20]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      999      1000      1001      1002      1003      1004      1005      1006 \n",
              "0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n",
              "     1007      1008      1009      1010      1011      1012      1013      1014 \n",
              "0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n",
              "     1015      1016      1017      1018 \n",
              "0.4935775 0.5030894 0.4978806 0.4886331 "
            ],
            "text/latex": "\\begin{description*}\n\\item[999] 0.490179249818258\n\\item[1000] 0.479218499099683\n\\item[1001] 0.466818479852065\n\\item[1002] 0.474001069455248\n\\item[1003] 0.492787663967445\n\\item[1004] 0.493856154997504\n\\item[1005] 0.495101564646223\n\\item[1006] 0.487286099421815\n\\item[1007] 0.490701348960405\n\\item[1008] 0.484402624071869\n\\item[1009] 0.490696276120968\n\\item[1010] 0.511998846261919\n\\item[1011] 0.489515226936648\n\\item[1012] 0.470676122211879\n\\item[1013] 0.474459285611829\n\\item[1014] 0.479958339148108\n\\item[1015] 0.493577529465861\n\\item[1016] 0.503089377118306\n\\item[1017] 0.497880612141404\n\\item[1018] 0.488633086516518\n\\end{description*}\n",
            "text/markdown": "999\n:   0.4901792498182581000\n:   0.4792184990996831001\n:   0.4668184798520651002\n:   0.4740010694552481003\n:   0.4927876639674451004\n:   0.4938561549975041005\n:   0.4951015646462231006\n:   0.4872860994218151007\n:   0.4907013489604051008\n:   0.4844026240718691009\n:   0.4906962761209681010\n:   0.5119988462619191011\n:   0.4895152269366481012\n:   0.4706761222118791013\n:   0.4744592856118291014\n:   0.4799583391481081015\n:   0.4935775294658611016\n:   0.5030893771183061017\n:   0.4978806121414041018\n:   0.488633086516518\n\n",
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>999</dt><dd>0.490179249818258</dd><dt>1000</dt><dd>0.479218499099683</dd><dt>1001</dt><dd>0.466818479852065</dd><dt>1002</dt><dd>0.474001069455248</dd><dt>1003</dt><dd>0.492787663967445</dd><dt>1004</dt><dd>0.493856154997504</dd><dt>1005</dt><dd>0.495101564646223</dd><dt>1006</dt><dd>0.487286099421815</dd><dt>1007</dt><dd>0.490701348960405</dd><dt>1008</dt><dd>0.484402624071869</dd><dt>1009</dt><dd>0.490696276120968</dd><dt>1010</dt><dd>0.511998846261919</dd><dt>1011</dt><dd>0.489515226936648</dd><dt>1012</dt><dd>0.470676122211879</dd><dt>1013</dt><dd>0.474459285611829</dd><dt>1014</dt><dd>0.479958339148108</dd><dt>1015</dt><dd>0.493577529465861</dd><dt>1016</dt><dd>0.503089377118306</dd><dt>1017</dt><dd>0.497880612141404</dd><dt>1018</dt><dd>0.488633086516518</dd></dl>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n",
              "[16] Up   Up   Down Up   Up  \n",
              "Levels: Down Up"
            ],
            "text/latex": "\\begin{enumerate*}\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Down\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Up\n\\item Down\n\\item Up\n\\item Up\n\\end{enumerate*}\n\n\\emph{Levels}: \\begin{enumerate*}\n\\item 'Down'\n\\item 'Up'\n\\end{enumerate*}\n",
            "text/markdown": "1. Up\n2. Up\n3. Up\n4. Up\n5. Up\n6. Up\n7. Up\n8. Up\n9. Up\n10. Up\n11. Up\n12. Down\n13. Up\n14. Up\n15. Up\n16. Up\n17. Up\n18. Down\n19. Up\n20. Up\n\n\n\n**Levels**: 1. 'Down'\n2. 'Up'\n\n\n",
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Down</li><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Up</li><li>Down</li><li>Up</li><li>Up</li></ol>\n",
              "\n",
              "<details>\n",
              "\t<summary style=display:list-item;cursor:pointer>\n",
              "\t\t<strong>Levels</strong>:\n",
              "\t</summary>\n",
              "\t<style>\n",
              "\t.list-inline {list-style: none; margin:0; padding: 0}\n",
              "\t.list-inline>li {display: inline-block}\n",
              "\t.list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "\t</style>\n",
              "\t<ol class=list-inline><li>'Down'</li><li>'Up'</li></ol>\n",
              "</details>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RduoLcz9pm6N"
      },
      "source": [
        "También podríamos utilizar un threshold distinto al 50% para hacer predicciones. Por ejemplo, supongamos que quremos predecir que el mercado baja solo si hay una probabilidad muy alta de que descienda en ese día. Por ejemplo, una probabilidad superior al 90%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "PadVIoEaPhn1",
        "outputId": "48ab6dfe-8ad0-4b64-9f7f-bf84a462dff4"
      },
      "source": [
        "sum(lda.pred$posterior [,1]>.9)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] 0"
            ],
            "text/latex": "0",
            "text/markdown": "0",
            "text/html": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axki_EFJp6RW"
      },
      "source": [
        "En este caso, no hay ningún día en 2005 que cumplan esta condición. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UfIgxn1lgJ8"
      },
      "source": [
        "# Referencias\r\n",
        "\r\n",
        "[rstudio-pubs-static.s3.amazonaws.com](https://rstudio-pubs-static.s3.amazonaws.com/389151_3cfc2588daff4989b0ce8da8b3d5ab01.html#an%C3%A1lisis_discriminante_lineal)\r\n",
        "\r\n",
        "[cienciadedatos.net](https://www.cienciadedatos.net/documentos/28_linear_discriminant_analysis_lda_y_quadratic_discriminant_analysis_qda)\r\n",
        "\r\n"
      ]
    }
  ]
}