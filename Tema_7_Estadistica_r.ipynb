{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tema 7 Estadistica_r.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidCarneros/notebooks_uned/blob/main/Tema_7_Estadistica_r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVf2KQZZXgLQ"
      },
      "source": [
        "# Análisis discriminante lineal\r\n",
        "\r\n",
        "El Análisis Discriminante Lineal (LDA, por sus siglas en inglés) es un método de clasificación supervisado orientado a problemas en los que la variable dependiente es categórica con más de dos clases. Haciendo uso del teorema de Bayes, LDA estima la probabilidad de que una observación, dado un determinado valor en los predictores, pertenezca a cada una de las clases de la variable respuesta: $Pr(Y = k | X = x)$. Finalmente, se asigna la observación a la clase $k$ para la que la probabilidad predicha es mayor.\r\n",
        "\r\n",
        "LDA es una alternativa a la regresión logística (tema 8), para el caso en el que la variable respuesta puede tomar más de dos valores. Aunque existen extensiones de la regresión logística para problemas de clasificación multiclase, el LDA presenta una serie de ventajas:\r\n",
        "\r\n",
        "- Si las clases están bien separadas, los parámetros estimados en el modelo de regresión logística son inestables. El método de LDA no sufre este problema.\r\n",
        "\r\n",
        "- Si el número de observaciones $n$ es pequeño, y los predictores siguen una distribución aproximadamente normal en cada una de las clases, LDA también es más estable que la regresión logística.\r\n",
        "\r\n",
        "No obstante, en problemas de clasificación binarios, ambos métodos suelen llegar a resultados similares. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMGvv3Z_bUnN"
      },
      "source": [
        "## Teorema de Bayes para clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pZVbFt-es67"
      },
      "source": [
        "El teorema de Bayes es útil para obtener probabilidades condicionales, ya que expresa la probabilidad condicional de un evento aleatorio A dado que otro evento B ya ha ocurrido. Esta probabilidad es igual a la probabilidad conjunta de A y B, $P(AB)$ entre la probabilidad marginal de B, $P(B)$:\r\n",
        "\r\n",
        "$$ P(A_j|B) = \\frac{P(A_jB)}{P(B)} = \\frac{P(A_j) \\cdot P(B|A_j)}{\\sum ^n _{i=1} P(A_i) \\cdot P(B|A_j)}$$\r\n",
        "\r\n",
        "\r\n",
        "En el contexto de la clasificación, mediante el teorema de Bayes se calcula la probabilidad de que la variable respuesta Y pertenezca a cada uno de los posibles niveles, dados unos determinados valores de los predictores. Supongamos que queremos clasificar una observación en una de $K$ clases, tal que $K \\geq 2$. Hay que considerar:\r\n",
        "\r\n",
        "- La **probabilidad a priori** (*prior probability*) de que un individuo tomado al azar pertenezca a la clase k-ésima, $\\pi _k$. En general, estimar $\\pi _k$ es fácil si tenemos una muestra aleatoria de valores de $Y$: solo tenemos que calcular la proporción de observaciones de entrenamiento que pertenecen a la clase k-ésima.\r\n",
        "\r\n",
        "$$ \\hat \\pi _k = \\frac{n_k}{n}$$\r\n",
        "\r\n",
        "- La **función de densidad de $X$** para una observación que pertenece a la clase k-ésima, definida como $f_k(x) = P(X = x | Y = k)$. Cuanto mayor sea $f_k(x)$, mayor será la probabilidad de que una observación de la clase k-ésima tome un valor $X \\approx x$. También deberemos estimar $f_k (x)$ a través de la muestra, ya que raramente disponemos los parámetros poblacionales. Esto hace que el clasificador LDA suponga una aproximación al clasificador de Bayes.\r\n",
        "\r\n",
        "El teorema de Bayes establece que:\r\n",
        "\r\n",
        "$$Pr(Y=k | X = x) = \\frac{\\pi _k f_k(x)}{\\sum ^K _{l=1} \\pi _l f_l(x)}$$\r\n",
        "\r\n",
        "\r\n",
        "Donde $p_k(x)$ o $Pr(Y=k | X = x)$ representa la  **probabilidad a posteri** (*posterior probability*), que es la probabilidad de que una observación $X=x$ pertenezca a la clase $k$.  Es decir, la probabilidad de la variable respuesta dado el valor del predictor (probabilidad condicional).  Cada observación se clasificará dentro del nivel que tiene la probabilidad $p_k(x)$ más alta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5EKppK4bcMk"
      },
      "source": [
        "## Análisis discriminante lineal para $p = 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5780qGLbcTb"
      },
      "source": [
        "A la hora de estimar $f_k(x)$ en un escenario con un solo predictor ($p=1$), deberemos contar con ciertas suposiciones sobre su forma. Si asumimos que $f_k(x)$ sigue una distribución normal, en un escenario unidimensional, la función de densidad normal tomaría la forma:\r\n",
        "\r\n",
        "$$f_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma _k}}\\text{exp}(-\\frac{1}{2\\sigma ^2_k}(x-\\mu _k)^2)$$\r\n",
        "\r\n",
        "Donde $\\mu _k$ es la media y $\\sigma ^2 _k$ es la varianza para la clase $k$. Por ahora, supongamos que $\\sigma ^2 _1 = ... = \\sigma ^2 _k$, es decir, todas las clases tienen la misma varianza, que para simplificar, podemos denotar como $\\sigma ^2$.  Incluyendo la fórmula anterior de $f_k(x)$ en la ecuación de Bayes, obtenemos:\r\n",
        "\r\n",
        "$$p_k(x) = \\frac{\\pi _k \\frac{1}{\\sqrt{2\\pi\\sigma}}\\text{exp}(-\\frac{1}{2\\sigma ^2}(x-\\mu _k)^2)}{\\sum ^K _{l=1} \\pi _l \\frac{1}{\\sqrt{2\\pi\\sigma}}\\text{exp}(-\\frac{1}{2\\sigma ^2}(x-\\mu _l)^2)}$$\r\n",
        "\r\n",
        "Mediante la transformación logarítmica de los términos de la ecuación anterior, y reagrupando los términos, podemos obtener una ecuación más simplificada:\r\n",
        "$$\\delta _k(x) = x \\cdot \\frac{\\mu _k}{\\sigma ^2} - \\frac{\\mu ^2 _k}{2 \\sigma ^2} + log(\\pi _k)$$\r\n",
        "\r\n",
        "donde $\\delta _k(x)$ es el log de $p_k (x)$.  Con esta última ecuación, el clasificador LDA asigna una observación $X = x$ a la clase que maximiza $\\delta _k(x)$. La función discriminante $\\delta _k(x)$  es lineal respecto de x, de ahí el término *lineal* en el nombre de LDA, y por ello, generará **límites de decisión lineales**. \r\n",
        "\r\n",
        "### Límites de decisión\r\n",
        "\r\n",
        "Supongamos que K = 2 y que la probabilidad a priori $\\pi _k$ es 0.5, es decir $\\pi _1 = \\pi _2$, el límite de decisión de Bayes vendría dado por:\r\n",
        "\r\n",
        "$$x = \\frac{\\mu ^2 _1 - \\mu ^2 _2}{2(\\mu _1 - \\mu _2)} = \\frac{\\mu _1 + \\mu _2}{2}$$\r\n",
        "\r\n",
        "Un ejemplo se muestra en el panel izquierdo de la Figura 4.4. Las dos funciones de densidad normales que se representan gráficamente, $f _1(x)$ y $f _2(x)$, se corresponden con dos clases distintas, con media $\\mu _1 = - 1,25$ y varianza $\\sigma ^2 _1 = 1$, y por otro lado, media $\\mu _2 = 1,25$ y varianza $\\sigma ^2 _2 = 1$. En este caso, el clasificador Bayes asignará la observación a la clase k=1 (verde) si $x \\lt 0$, y a la clase k=2 (rojo) si $x \\gt 0$.\r\n",
        "\r\n",
        "Sin embargo, en la práctica no conocemos los parámetros poblacionales, aunque tengamos cierta seguridad de que X se distribuye de forma normal para cada clase. Por ello, no podemos calcular el límite de decisión de Bayes. En este caso, debemos obtener las estimaciones de los parámetros  $\\mu _1= ... = \\mu _k$, $\\sigma ^2$, y $\\pi _1=... = \\pi _k$ a partir de la muestra. En concreto, el análisis discrminante lineal se basa en las siguientes estimaciones:\r\n",
        "\r\n",
        "$$\\hat \\mu _k = \\frac{1}{n_k} \\sum _{i:y_i=k} x _i$$\r\n",
        "\r\n",
        "$$\\hat \\sigma ^2 = \\frac{1}{n - K} \\sum _{k=1} ^K \\sum _{i:y_i=k} (x_i - \\hat \\mu _k)^2$$\r\n",
        "\r\n",
        "Donde $n$ es el número total de observaciones de entrenamiento, y $n _k$ es el número de observaciones de entrenamiento para la clase $k$. La estimación de $\\mu _k$ es simplemente la media de todas las observaciones para la clase $k$. Como ya hemos comentado, LDA estima $\\pi _k$ como la proporción de instancias de entrenamiento que pertenecen a la clase k-ésima. \r\n",
        "\r\n",
        "El clasificador LDA utiliza estas estimaciones para asignar la observación $X = x$ a la clase para la que \r\n",
        "\r\n",
        "$$\\hat \\delta _k(x) = x \\cdot \\frac{\\hat \\mu _k}{\\hat \\sigma ^2} - \\frac{\\hat \\mu ^2 _k}{2 \\hat \\sigma ^2} + log(\\hat \\pi _k)$$\r\n",
        "\r\n",
        "es mayor.\r\n",
        "\r\n",
        "El panel derecho de la Figura 4.4 muestra un histograma para una muestra de 20 observaciones para cada clase ($n_1 = n_2 = 20$). Para implementar LDA, empezamos estimado los parámetros $\\pi _k$, $\\mu _k$ y $\\sigma ^2$. \r\n",
        "\r\n",
        "En este caso, dado que $n_1=n2$, tenemos $\\pi _1= \\pi _2$, con lo que en esta situación el límite de decisión corresponderá al punto medio de la media de las dos clases. En la figura podemos ver que el límite de decisión LDA está ligeramente desplazado hacia la derecha en relación al límite de decisión de Bayes, pero sigue siendo una aproximación bastante buena.\r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/cfeed1589da51617c6f9f5b6c278f31b.png\" width=\"80%\"/>\r\n",
        "<figcaption>Figura 4.4. Izquierda: Se muestran dos funciones de densidad bidimensionales. Las líneas de puntos representan el límite de decisión de Bayes. En la parte derecha, se dibujan 20 observaciones de cada clase como histogramas. </figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IG6Oc58bkx8"
      },
      "source": [
        "## Análisis discriminante lineal para $p \\gt 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9tBu_NoGY-n"
      },
      "source": [
        "  Podemos extender el LDA para casos con más de un predictor. En este caso ha de asumirse que $X = (X_1, X_2,...,X_p)$ siguen una **distribución normal multivariante**, con media $\\mu _k$ específica de la clase y varianza $\\sigma ^2$ común entre clases. \r\n",
        "\r\n",
        "\r\n",
        "  ### Distribución normal multivariante\r\n",
        "\r\n",
        "La distribución normal multivariante asume que cada uno de los predictores se distribuyen de forma normal, con una cierta correlación entre cada par de predictores. Las imágenes de la Figura 4.5 muestran representaciones gráficas de distribuciones normales multivariante de 2 elementos (distribución normal bivariante).\r\n",
        "\r\n",
        "La altura de la superficie en un punto concreto representa la probabilidad de que tanto $X_1$ como $X_2$ caigan en una pequeña región alrededor del punto. En el ejemplo de la izquierda se observa una forma de campana debido a que $Var(X_1) = Var(X_2)$ y $Cor(X_1, X_2) = 0$. Sin embargo, esta forma de campana se distorsiona si los predictores están correlacionados o no tienen igualdad de varianzas. Esto se ilustra en el panel derecho de la Figura 4.5. \r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/50a896b2463f6e0f5f3c1489afd75e79.png\" width=\"80%\"/>\r\n",
        "<figcaption>Figura 4.5. Dos funciones de densidad con $p=2$. En la izquierda, los predictores no están correlacionados. En la derecha, los predictores tienen una correlación de 0.7  </figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n",
        "\r\n",
        "Se utiliza el término $X \\sim N (\\mu, \\sum)$ para indicar que una variable aleatoria $X$ p-dimensional sigue una distribución normal multivariante, donde $\\mu$ es su media y donde $\\sum$ es su matriz de covarianza $p \\times p$ común a todas las clases $L$. La ecuación se define como:\r\n",
        "\r\n",
        "$$f(x) = \\frac{1}{(2\\pi)^{p/2}|\\sum|^{1/2}} \\ exp \\ (-\\frac{1}{2}(x-\\mu)^T\\sum\\ ^{-1} \\ (x-\\mu))$$\r\n",
        "\r\n",
        " Introduciendo $f(x)$ en la ecuación de Bayes, obtenemos un clasificador que asignará la observación $X = x$ a la clase para la que se maximice la siguiente ecuación:\r\n",
        "\r\n",
        "$${\\delta}_k(x) = x^T \\ \\sum \\ ^{-1} \\  \\mu_k -\\ \\frac{1}{2} \\mu_k^T\\sum \\ ^{-1} \\ \\mu_k + log(\\pi_k)$$\r\n",
        "\r\n",
        "\r\n",
        "Se muestra un ejemplo en el panel izquierdo de la Figura 4.6 con tres clases Gaussianas con variazan común. Las tres elipses representan las regiones que contienen el 95% de la probabilidad para cada una de las tres clases. Las líneas de puntos son los límites de decisión. En otras palabras, representan el conjunto de valores de $x$ para los cuales $\\delta _k (x) = \\delta _l(x)$. Es decir,\r\n",
        "\r\n",
        "$$ x^T \\ \\sum \\ ^{-1} \\  \\mu_k -\\ \\frac{1}{2} \\mu_k^T\\sum \\ ^{-1} \\ \\mu_k =  x^T \\ \\sum \\ ^{-1} \\  \\mu_l -\\ \\frac{1}{2} \\mu_l^T\\sum \\ ^{-1} \\ \\mu_l $$\r\n",
        "\r\n",
        "para $k \\neq l$. En este caso, el término logarítmico ha desaparecido porque las tres clases tienen el mismo número de observaciones. Por tanto, $\\pi _k$ es igual para las tres clases. \r\n",
        "\r\n",
        "De nuevo, desconociendo los parámetros poblacionales no podemos obtener el límite de decisión de Bayes, por lo que tendremos que estimar $\\mu _1=...=\\mu _k$, $\\pi _1=...=\\pi _k$ y $\\sum$ para obtener los límites de decisión de LDA, que dividen el espacio del predictor en regiones correspondiente a las clases. \r\n",
        "\r\n",
        "En el panel derecho de la Figura 4-6, se representan 20 observaciones para cada una de las clases, y los límites de decisión correspondientes. Nuevamente, los límites de decisión de LDA son muy cercanos a los de Bayes. \r\n",
        "\r\n",
        "<center>\r\n",
        "<p>\r\n",
        "<img src=\"https://i.gyazo.com/0ee9361291212d8f5c06d81c1f38adcc.png\" width=\"80%\"/>\r\n",
        "<figcaption>Figura 4.6.  </figcaption>\r\n",
        "</p>\r\n",
        "</center>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCY1sWyabyxc"
      },
      "source": [
        "## Lab: Análisis discriminante lineal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7otLY89Pb2qL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UfIgxn1lgJ8"
      },
      "source": [
        "# Referencias\r\n",
        "\r\n",
        "[rstudio-pubs-static.s3.amazonaws.com](https://rstudio-pubs-static.s3.amazonaws.com/389151_3cfc2588daff4989b0ce8da8b3d5ab01.html#an%C3%A1lisis_discriminante_lineal)\r\n",
        "\r\n",
        "[cienciadedatos.net](https://www.cienciadedatos.net/documentos/28_linear_discriminant_analysis_lda_y_quadratic_discriminant_analysis_qda)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTTXGe3tlw2D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}